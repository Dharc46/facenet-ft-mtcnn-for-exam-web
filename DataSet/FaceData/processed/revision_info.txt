arguments: src/align_dataset_mtcnn.py Dataset/FaceData/raw Dataset/FaceData/processed --image_size 160 --margin 32 --random_order --gpu_memory_fraction 0.25
--------------------
tensorflow version: 2.13.0
--------------------
git hash: b'69ff1e149c0d84a123d6516ddd82970e65392608'
--------------------
b'diff --git a/src/align/detect_face.py b/src/align/detect_face.py\nindex 2300ff3..51ec24b 100644\n--- a/src/align/detect_face.py\n+++ b/src/align/detect_face.py\n@@ -30,7 +30,6 @@ from six import string_types, iteritems\n \n import numpy as np\n import tensorflow as tf\n-#from math import floor\n import cv2\n import os\n \n@@ -777,5 +776,4 @@ def imresample(img, sz):\n #         for a2 in range(0,ws):\n #             for a3 in range(0,3):\n #                 im_data[a1,a2,a3] = img[int(floor(a1*dy)),int(floor(a2*dx)),a3]\n-#     return im_data\n-\n+#     return im_data\n\\ No newline at end of file\ndiff --git a/src/face_rec_cam.py b/src/face_rec_cam.py\nindex 1a425a5..b0bc64f 100644\n--- a/src/face_rec_cam.py\n+++ b/src/face_rec_cam.py\n@@ -1,11 +1,5 @@\n-from __future__ import absolute_import\n-from __future__ import division\n-from __future__ import print_function\n-\n import tensorflow as tf\n from imutils.video import VideoStream\n-\n-\n import argparse\n import facenet\n import imutils\n@@ -19,7 +13,6 @@ import cv2\n import collections\n from sklearn.svm import SVC\n \n-\n def main():\n     parser = argparse.ArgumentParser()\n     parser.add_argument(\'--path\', help=\'Path of the video you want to test on.\', default=0)\n@@ -39,43 +32,44 @@ def main():\n         model, class_names = pickle.load(file)\n     print("Custom Classifier, Successfully loaded")\n \n-    with tf.Graph().as_default():\n-\n-        # Cai dat GPU neu co\n-        gpu_options = tf.compat.v1.GPUOptions(per_process_gpu_memory_fraction=0.6)\n-        sess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(gpu_options=gpu_options, log_device_placement=False))\n+    # Use TensorFlow 2.x features\n+    physical_devices = tf.config.list_physical_devices(\'GPU\')\n+    if physical_devices:\n+        tf.config.set_logical_device_configuration(physical_devices[0], [tf.config.LogicalDeviceConfiguration(memory_limit=4096)])\n \n-        with sess.as_default():\n-\n-            # Load the model\n+    # Create the session and load the model\n+    with tf.Graph().as_default():\n+        with tf.compat.v1.Session() as sess:\n+            # Load the facenet model\n             print(\'Loading feature extraction model\')\n             facenet.load_model(FACENET_MODEL_PATH)\n \n             # Get input and output tensors\n-            images_placeholder = tf.get_default_graph().get_tensor_by_name("input:0")\n-            embeddings = tf.get_default_graph().get_tensor_by_name("embeddings:0")\n-            phase_train_placeholder = tf.get_default_graph().get_tensor_by_name("phase_train:0")\n-            embedding_size = embeddings.get_shape()[1]\n+            images_placeholder = tf.compat.v1.get_default_graph().get_tensor_by_name("input:0")\n+            embeddings = tf.compat.v1.get_default_graph().get_tensor_by_name("embeddings:0")\n+            phase_train_placeholder = tf.compat.v1.get_default_graph().get_tensor_by_name("phase_train:0")\n+            embedding_size = embeddings.shape[1]\n \n+            # Initialize MTCNN\n             pnet, rnet, onet = align.detect_face.create_mtcnn(sess, "src/align")\n \n             people_detected = set()\n             person_detected = collections.Counter()\n \n-            cap  = VideoStream(src=0).start()\n+            cap = VideoStream(src=0).start()\n \n-            while (True):\n+            while True:\n                 frame = cap.read()\n                 frame = imutils.resize(frame, width=600)\n                 frame = cv2.flip(frame, 1)\n \n+                # Detect faces\n                 bounding_boxes, _ = align.detect_face.detect_face(frame, MINSIZE, pnet, rnet, onet, THRESHOLD, FACTOR)\n \n                 faces_found = bounding_boxes.shape[0]\n                 try:\n                     if faces_found > 1:\n-                        cv2.putText(frame, "Only one face", (0, 100), cv2.FONT_HERSHEY_COMPLEX_SMALL,\n-                                    1, (255, 255, 255), thickness=1, lineType=2)\n+                        cv2.putText(frame, "Only one face", (0, 100), cv2.FONT_HERSHEY_COMPLEX_SMALL, 1, (255, 255, 255), thickness=1, lineType=2)\n                     elif faces_found > 0:\n                         det = bounding_boxes[:, 0:4]\n                         bb = np.zeros((faces_found, 4), dtype=np.int32)\n@@ -84,27 +78,27 @@ def main():\n                             bb[i][1] = det[i][1]\n                             bb[i][2] = det[i][2]\n                             bb[i][3] = det[i][3]\n-                            print(bb[i][3]-bb[i][1])\n+                            print(bb[i][3] - bb[i][1])\n                             print(frame.shape[0])\n-                            print((bb[i][3]-bb[i][1])/frame.shape[0])\n-                            if (bb[i][3]-bb[i][1])/frame.shape[0]>0.25:\n+                            print((bb[i][3] - bb[i][1]) / frame.shape[0])\n+                            if (bb[i][3] - bb[i][1]) / frame.shape[0] > 0.25:\n                                 cropped = frame[bb[i][1]:bb[i][3], bb[i][0]:bb[i][2], :]\n-                                scaled = cv2.resize(cropped, (INPUT_IMAGE_SIZE, INPUT_IMAGE_SIZE),\n-                                                    interpolation=cv2.INTER_CUBIC)\n+                                scaled = cv2.resize(cropped, (INPUT_IMAGE_SIZE, INPUT_IMAGE_SIZE), interpolation=cv2.INTER_CUBIC)\n                                 scaled = facenet.prewhiten(scaled)\n                                 scaled_reshape = scaled.reshape(-1, INPUT_IMAGE_SIZE, INPUT_IMAGE_SIZE, 3)\n+\n+                                # Run the model to get embeddings\n                                 feed_dict = {images_placeholder: scaled_reshape, phase_train_placeholder: False}\n                                 emb_array = sess.run(embeddings, feed_dict=feed_dict)\n \n+                                # Predict the class of the detected face\n                                 predictions = model.predict_proba(emb_array)\n                                 best_class_indices = np.argmax(predictions, axis=1)\n-                                best_class_probabilities = predictions[\n-                                    np.arange(len(best_class_indices)), best_class_indices]\n+                                best_class_probabilities = predictions[np.arange(len(best_class_indices)), best_class_indices]\n                                 best_name = class_names[best_class_indices[0]]\n-                                print("Name: {}, Probability: {}".format(best_name, best_class_probabilities))\n-\n-\n+                                print(f"Name: {best_name}, Probability: {best_class_probabilities}")\n \n+                                # If confidence is high enough, label the face\n                                 if best_class_probabilities > 0.8:\n                                     cv2.rectangle(frame, (bb[i][0], bb[i][1]), (bb[i][2], bb[i][3]), (0, 255, 0), 2)\n                                     text_x = bb[i][0]\n@@ -120,7 +114,8 @@ def main():\n                                 else:\n                                     name = "Unknown"\n \n-                except:\n+                except Exception as e:\n+                    print(e)\n                     pass\n \n                 cv2.imshow(\'Face Recognition\', frame)\n@@ -130,5 +125,5 @@ def main():\n             cap.release()\n             cv2.destroyAllWindows()\n \n-\n-main()\n+if __name__ == \'__main__\':\n+    main()\ndiff --git a/src/facenet.py b/src/facenet.py\nindex 26a4e3d..2b89fbc 100644\n--- a/src/facenet.py\n+++ b/src/facenet.py\n@@ -331,11 +331,14 @@ def get_dataset(path, has_class_directories=True):\n     return dataset\n \n def get_image_paths(facedir):\n+    valid_extensions = [\'.jpg\', \'.png\']  # Th\xc3\xaam c\xc3\xa1c \xc4\x91\xe1\xbb\x8bnh d\xe1\xba\xa1ng kh\xc3\xa1c n\xe1\xba\xbfu c\xe1\xba\xa7n\n     image_paths = []\n     if os.path.isdir(facedir):\n         images = os.listdir(facedir)\n-        image_paths = [os.path.join(facedir,img) for img in images]\n+        image_paths = [os.path.join(facedir, img) for img in images \n+                       if os.path.splitext(img)[1].lower() in valid_extensions]\n     return image_paths\n+\n   \n def split_dataset(dataset, split_ratio, min_nrof_images_per_class, mode):\n     if mode==\'SPLIT_CLASSES\':'